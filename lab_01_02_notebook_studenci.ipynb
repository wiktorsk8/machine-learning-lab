{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1.2: Feature Engineering i Preprocessing Danych\n",
    "\n",
    "**Sztuczna Inteligencja - Semestr V**\n",
    "\n",
    "**ProwadzƒÖcy:** ≈Åukasz Grala\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Cele laboratorium:\n",
    "\n",
    "1. ‚úÖ Nauka preprocessingu danych (missing values, outliers)\n",
    "2. ‚úÖ Feature engineering (tworzenie nowych cech)\n",
    "3. ‚úÖ Kodowanie zmiennych kategorycznych\n",
    "4. ‚úÖ Skalowanie i normalizacja\n",
    "5. ‚úÖ Pierwsze modele ML (scikit-learn)\n",
    "6. ‚úÖ Train-test split i ewaluacja\n",
    "\n",
    "---\n",
    "\n",
    "## üìö ≈πr√≥d≈Ça danych:\n",
    "\n",
    "- **Titanic Dataset** (klasyfikacja - survival)\n",
    "- **Boston Housing** (regresja - ceny)\n",
    "- **UCI ML Repository**\n",
    "- **Kaggle Datasets**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Importy i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Biblioteki za≈Çadowane!\n",
      "NumPy: 2.3.4\n",
      "Pandas: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Podstawowe biblioteki\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn - preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Scikit-learn - modele\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Ustawienia\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Wy≈ÇƒÖcz warningi\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Biblioteki za≈Çadowane!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CZƒò≈öƒÜ 1: Missing Values i Outliers\n",
    "\n",
    "### Dataset: Titanic\n",
    "\n",
    "**Cel:** Przewidzieƒá czy pasa≈ºer prze≈ºy≈Ç katastrofƒô\n",
    "\n",
    "**Features:**\n",
    "- `survived`: 0 = No, 1 = Yes (TARGET)\n",
    "- `pclass`: Klasa biletu (1, 2, 3)\n",
    "- `sex`: P≈Çeƒá\n",
    "- `age`: Wiek\n",
    "- `sibsp`: Liczba rodze≈Ñstwa/ma≈Ç≈ºonk√≥w na pok≈Çadzie\n",
    "- `parch`: Liczba rodzic√≥w/dzieci na pok≈Çadzie\n",
    "- `fare`: Cena biletu\n",
    "- `embarked`: Port zaokrƒôtowania (C, Q, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Titanic:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Wymiary: (891, 12)\n",
      "\n",
      "Kolumny: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie danych Titanic\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset Titanic:\")\n",
    "print(titanic.head())\n",
    "print(f\"\\nWymiary: {titanic.shape}\")\n",
    "print(f\"\\nKolumny: {titanic.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1.1: Analiza brakujƒÖcych danych \n",
    "**Cel:** Zidentyfikowaƒá i zwizualizowaƒá missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "             Missing_Count  Missing_Percent\n",
      "Cabin                  687        77.104377\n",
      "Age                    177        19.865320\n",
      "Embarked                 2         0.224467\n",
      "PassengerId              0         0.000000\n",
      "Name                     0         0.000000\n",
      "Pclass                   0         0.000000\n",
      "Survived                 0         0.000000\n",
      "Sex                      0         0.000000\n",
      "Parch                    0         0.000000\n",
      "SibSp                    0         0.000000\n",
      "Fare                     0         0.000000\n",
      "Ticket                   0         0.000000\n"
     ]
    }
   ],
   "source": [
    "# TODO 1: Sprawd≈∫ informacje o datasecie\n",
    "# U≈ºyj .info() aby zobaczyƒá typy danych i non-null counts\n",
    "titanic.info()\n",
    "\n",
    "# TODO 2: Policz missing values w ka≈ºdej kolumnie\n",
    "# U≈ºyj .isnull().sum()\n",
    "missing_values = titanic.isnull().sum()\n",
    "\n",
    "# TODO 3: Oblicz procent brakujƒÖcych danych\n",
    "missing_percent = (missing_values / len(titanic)) * 100\n",
    "\n",
    "# TODO 4: Stw√≥rz DataFrame z podsumowaniem\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "# Posortuj malejƒÖco i wy≈õwietl\n",
    "missing_df = missing_df.sort_values(by='Missing_Count', ascending=False)\n",
    "print(missing_df)\n",
    "# TODO 5: Wizualizacja missing values\n",
    "# Stw√≥rz heatmap pokazujƒÖcƒÖ missing values\n",
    "# Wskaz√≥wka: u≈ºyj sns.heatmap() z titanic.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Obs≈Çuga Missing Values \n",
    "\n",
    "**Strategie:**\n",
    "- **Age:** Imputacja medianƒÖ (wiele brakujƒÖcych, wa≈ºna cecha)\n",
    "- **Embarked:** Imputacja modƒÖ (ma≈Ço brakujƒÖcych)\n",
    "- **Cabin:** Usu≈Ñ kolumnƒô (>77% brakujƒÖcych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Usu≈Ñ kolumnƒô 'Cabin' (zbyt wiele missing values)\n",
    "titanic_clean = titanic.copy()\n",
    "# Tw√≥j kod:\n",
    "\n",
    "\n",
    "# TODO 2: Wype≈Çnij brakujƒÖce warto≈õci 'Age' medianƒÖ\n",
    "# Policz medianƒô\n",
    "age_median = # Tw√≥j kod\n",
    "# Wype≈Çnij brakujƒÖce\n",
    "\n",
    "\n",
    "# TODO 3: Wype≈Çnij brakujƒÖce warto≈õci 'Embarked' modƒÖ (najczƒôstsza warto≈õƒá)\n",
    "embarked_mode = # Tw√≥j kod (u≈ºyj .mode()[0])\n",
    "# Wype≈Çnij brakujƒÖce\n",
    "\n",
    "\n",
    "# TODO 4: Sprawd≈∫ czy wszystkie missing values zosta≈Çy obs≈Çu≈ºone\n",
    "print(\"\\nBrakujƒÖce warto≈õci po czyszczeniu:\")\n",
    "print(titanic_clean.isnull().sum())\n",
    "\n",
    "# TODO 5: Alternatywnie - u≈ºyj SimpleImputer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# titanic_clean[['Age']] = imputer.fit_transform(titanic_clean[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Wykrywanie Outliers \n",
    "\n",
    "**Metody:**\n",
    "- IQR (Interquartile Range)\n",
    "- Z-score\n",
    "- Wizualizacja (Box plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Wizualizacja outliers w kolumnie 'Fare'\n",
    "# Stw√≥rz box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Tw√≥j kod: sns.boxplot()\n",
    "\n",
    "plt.title('Fare Distribution - Box Plot')\n",
    "plt.show()\n",
    "\n",
    "# TODO 2: Wykryj outliers metodƒÖ IQR\n",
    "Q1 = titanic_clean['Fare'].quantile(0.25)\n",
    "Q3 = titanic_clean['Fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Granice outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# TODO 3: Zidentyfikuj outliers\n",
    "outliers = titanic_clean[(titanic_clean['Fare'] < lower_bound) | \n",
    "                         (titanic_clean['Fare'] > upper_bound)]\n",
    "\n",
    "print(f\"\\nLiczba outliers w 'Fare': {len(outliers)}\")\n",
    "print(f\"Procent outliers: {len(outliers)/len(titanic_clean)*100:.2f}%\")\n",
    "\n",
    "# TODO 4: Decyzja - co zrobiƒá z outliers?\n",
    "# Opcja A: Zachowaj (mogƒÖ byƒá prawdziwe - First Class)\n",
    "# Opcja B: Cap do percentyla 95\n",
    "# Opcja C: Transformacja log\n",
    "\n",
    "# Przyk≈Çad cappingu:\n",
    "# fare_95 = titanic_clean['Fare'].quantile(0.95)\n",
    "# titanic_clean['Fare'] = titanic_clean['Fare'].clip(upper=fare_95)\n",
    "\n",
    "# TODO 5: Por√≥wnaj rozk≈Çad przed i po transformacji\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Oryginalny\n",
    "axes[0].hist(titanic_clean['Fare'], bins=50, color='blue', alpha=0.7)\n",
    "axes[0].set_title('Original Fare Distribution')\n",
    "axes[0].set_xlabel('Fare')\n",
    "\n",
    "# Po log transform (przyk≈Çad)\n",
    "axes[1].hist(np.log1p(titanic_clean['Fare']), bins=50, color='green', alpha=0.7)\n",
    "axes[1].set_title('Log Transformed Fare')\n",
    "axes[1].set_xlabel('log(Fare + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CZƒò≈öƒÜ 2: Feature Engineering \n",
    "\n",
    "### Kodowanie zmiennych kategorycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2.1: Label Encoding (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Zakoduj kolumnƒô 'Sex' (male/female ‚Üí 0/1)\n",
    "# Metoda 1: Rƒôcznie z map()\n",
    "titanic_clean['Sex_encoded'] = titanic_clean['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# TODO 2: Alternatywnie - u≈ºyj LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Tw√≥j kod:\n",
    "\n",
    "\n",
    "# TODO 3: Sprawd≈∫ wynik\n",
    "print(\"\\nPor√≥wnanie oryginalnej i zakodowanej kolumny:\")\n",
    "print(titanic_clean[['Sex', 'Sex_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: One-Hot Encoding \n",
    "\n",
    "**Kiedy u≈ºywaƒá:** Dla zmiennych nominalnych (bez kolejno≈õci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: One-hot encode kolumny 'Embarked'\n",
    "# Metoda 1: pd.get_dummies()\n",
    "embarked_dummies = pd.get_dummies(titanic_clean['Embarked'], \n",
    "                                   prefix='Embarked',\n",
    "                                   drop_first=True)  # Unikaj dummy variable trap\n",
    "\n",
    "# TODO 2: Dodaj nowe kolumny do DataFrame\n",
    "titanic_clean = pd.concat([titanic_clean, embarked_dummies], axis=1)\n",
    "\n",
    "# TODO 3: Sprawd≈∫ wynik\n",
    "print(\"\\nOne-hot encoded 'Embarked':\")\n",
    "print(titanic_clean[['Embarked', 'Embarked_Q', 'Embarked_S']].head(10))\n",
    "\n",
    "# TODO 4: One-hot encode 'Pclass'\n",
    "# Tw√≥j kod:\n",
    "\n",
    "\n",
    "# TODO 5: Alternatywnie - u≈ºyj sklearn OneHotEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "# encoded = ohe.fit_transform(titanic_clean[['Embarked']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Tworzenie nowych Features \n",
    "**Feature Engineering to sztuka!** Kreatywno≈õƒá jest kluczowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Stw√≥rz feature 'FamilySize'\n",
    "# FamilySize = SibSp + Parch + 1 (pasa≈ºer sam)\n",
    "titanic_clean['FamilySize'] = # Tw√≥j kod\n",
    "\n",
    "# TODO 2: Stw√≥rz feature 'IsAlone'\n",
    "# IsAlone = 1 je≈õli FamilySize == 1, inaczej 0\n",
    "titanic_clean['IsAlone'] = # Tw√≥j kod\n",
    "\n",
    "# TODO 3: Stw√≥rz feature 'Title' z kolumny 'Name'\n",
    "# WyciƒÖgnij tytu≈Ç (Mr., Mrs., Miss., itp.) z imienia\n",
    "# Wskaz√≥wka: u≈ºyj str.extract() z regex\n",
    "titanic_clean['Title'] = titanic_clean['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# TODO 4: Zgrupuj rzadkie tytu≈Çy\n",
    "# ZastƒÖp rzadkie tytu≈Çy przez 'Rare'\n",
    "title_counts = titanic_clean['Title'].value_counts()\n",
    "rare_titles = title_counts[title_counts < 10].index\n",
    "titanic_clean['Title'] = titanic_clean['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "print(\"\\nRozk≈Çad tytu≈Ç√≥w:\")\n",
    "print(titanic_clean['Title'].value_counts())\n",
    "\n",
    "# TODO 5: Stw√≥rz feature 'AgeGroup'\n",
    "# Podziel wiek na grupy: Child (<16), Adult (16-60), Senior (>60)\n",
    "titanic_clean['AgeGroup'] = pd.cut(titanic_clean['Age'], \n",
    "                                    bins=[0, 16, 60, 100],\n",
    "                                    labels=['Child', 'Adult', 'Senior'])\n",
    "\n",
    "# TODO 6: Stw√≥rz feature 'FarePerPerson'\n",
    "# Je≈õli pasa≈ºer podr√≥≈ºuje z rodzinƒÖ, podziel cenƒô biletu\n",
    "titanic_clean['FarePerPerson'] = # Tw√≥j kod (Fare / FamilySize)\n",
    "\n",
    "# TODO 7: Analiza nowych features\n",
    "print(\"\\nNowe features:\")\n",
    "print(titanic_clean[['FamilySize', 'IsAlone', 'Title', 'AgeGroup', 'FarePerPerson']].head(10))\n",
    "\n",
    "# TODO 8: Sprawd≈∫ korelacjƒô nowych features z 'Survived'\n",
    "correlation = titanic_clean[['Survived', 'FamilySize', 'IsAlone']].corr()\n",
    "print(\"\\nKorelacja z Survived:\")\n",
    "print(correlation['Survived'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CZƒò≈öƒÜ 3: Skalowanie i Normalizacja "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3.1: Por√≥wnanie metod skalowania\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybierzmy cechy numeryczne do skalowania\n",
    "numeric_features = ['Age', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "\n",
    "# TODO 1: Standardization (StandardScaler)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "# Tw√≥j kod - fit_transform na numeric_features\n",
    "scaled_std = # Tw√≥j kod\n",
    "\n",
    "# TODO 2: Normalization (MinMaxScaler)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "# Tw√≥j kod\n",
    "scaled_minmax = # Tw√≥j kod\n",
    "\n",
    "# TODO 3: Por√≥wnanie rozk≈Çad√≥w\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# Oryginalne\n",
    "axes[0, 0].hist(titanic_clean['Age'].dropna(), bins=30, color='blue', alpha=0.7)\n",
    "axes[0, 0].set_title('Original Age')\n",
    "axes[0, 1].hist(titanic_clean['Fare'], bins=30, color='blue', alpha=0.7)\n",
    "axes[0, 1].set_title('Original Fare')\n",
    "\n",
    "# Standardized\n",
    "axes[1, 0].hist(scaled_std[:, 0], bins=30, color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('Standardized Age (mean=0, std=1)')\n",
    "axes[1, 1].hist(scaled_std[:, 1], bins=30, color='green', alpha=0.7)\n",
    "axes[1, 1].set_title('Standardized Fare')\n",
    "\n",
    "# Normalized\n",
    "axes[2, 0].hist(scaled_minmax[:, 0], bins=30, color='red', alpha=0.7)\n",
    "axes[2, 0].set_title('Normalized Age (0-1)')\n",
    "axes[2, 1].hist(scaled_minmax[:, 1], bins=30, color='red', alpha=0.7)\n",
    "axes[2, 1].set_title('Normalized Fare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO 4: Sprawd≈∫ statystyki\n",
    "print(\"\\nStatystyki po standardizacji:\")\n",
    "print(f\"Mean Age: {scaled_std[:, 0].mean():.6f}\")  # Powinno byƒá ~0\n",
    "print(f\"Std Age: {scaled_std[:, 0].std():.6f}\")    # Powinno byƒá ~1\n",
    "\n",
    "print(\"\\nStatystyki po normalizacji:\")\n",
    "print(f\"Min Age: {scaled_minmax[:, 0].min():.6f}\")  # Powinno byƒá 0\n",
    "print(f\"Max Age: {scaled_minmax[:, 0].max():.6f}\")  # Powinno byƒá 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3.2: Kiedy kt√≥rej metody u≈ºyƒá?\n",
    "\n",
    "**Eksperyment:** Por√≥wnaj wp≈Çyw skalowania na model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Przygotuj dane do modelowania\n",
    "# Wybierzmy proste features do demonstracji\n",
    "features_for_model = ['Age', 'Fare', 'Sex_encoded', 'Pclass']\n",
    "X = titanic_clean[features_for_model].copy()\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "# Train-test split (o tym wiƒôcej w nastƒôpnej czƒô≈õci)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO 1: Model BEZ skalowania\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_no_scale = LogisticRegression(max_iter=1000)\n",
    "# Trenuj i ewaluuj\n",
    "\n",
    "\n",
    "# TODO 2: Model ZE standardizacjƒÖ\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Uwaga: tylko transform!\n",
    "\n",
    "model_with_scale = LogisticRegression(max_iter=1000)\n",
    "# Trenuj i ewaluuj\n",
    "\n",
    "\n",
    "# TODO 3: Por√≥wnaj wyniki\n",
    "print(\"\\nPor√≥wnanie accuracy:\")\n",
    "print(f\"Bez skalowania: {accuracy_no_scale:.4f}\")\n",
    "print(f\"Ze skalowaniem: {accuracy_with_scale:.4f}\")\n",
    "print(f\"\\nR√≥≈ºnica: {abs(accuracy_with_scale - accuracy_no_scale):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CZƒò≈öƒÜ 4: Pierwszy Model ML\n",
    "\n",
    "### Scikit-learn Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4.1: Train-Test Split \n",
    "\n",
    "**Kluczowa zasada ML:** NIGDY nie trenuj na danych testowych!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Przygotuj finalne features\n",
    "# Wybierz najlepsze features na podstawie analizy\n",
    "feature_columns = [\n",
    "    'Age', 'Fare', 'FamilySize', 'IsAlone', 'FarePerPerson',\n",
    "    'Sex_encoded', 'Pclass',\n",
    "    'Embarked_Q', 'Embarked_S'\n",
    "]\n",
    "\n",
    "X = titanic_clean[feature_columns].copy()\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# TODO 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        # 20% na test\n",
    "    random_state=42,      # Reprodukowalno≈õƒá\n",
    "    stratify=y            # Zachowaj proporcje klas\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# TODO 3: Sprawd≈∫ balans klas\n",
    "print(\"\\nBalans klas w train set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nBalans klas w test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# TODO 4: Skalowanie (WA≈ªNE: fit tylko na train!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit + transform na train\n",
    "X_test_scaled = scaler.transform(X_test)        # Tylko transform na test!\n",
    "\n",
    "print(\"\\n‚úì Dane przygotowane do modelowania!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4.2: Logistic Regression\n",
    "\n",
    "**Cel:** Przewidzieƒá czy pasa≈ºer prze≈ºy≈Ç (klasyfikacja binarna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Stw√≥rz i wytrenuj model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Trenuj model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úì Model wytrenowany!\")\n",
    "\n",
    "# TODO 2: Predykcje\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "# TODO 3: Prawdopodobie≈Ñstwa\n",
    "y_proba_test = model.predict_proba(X_test_scaled)\n",
    "print(\"\\nPrzyk≈Çadowe prawdopodobie≈Ñstwa (pierwsze 5 obserwacji):\")\n",
    "print(\"Prob(0), Prob(1)\")\n",
    "print(y_proba_test[:5])\n",
    "\n",
    "# TODO 4: Ewaluacja - Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n=== ACCURACY ===\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# TODO 5: Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Died', 'Survived'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# TODO 6: Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Died', 'Survived']))\n",
    "\n",
    "# TODO 7: Feature Importance\n",
    "# Wsp√≥≈Çczynniki regresji logistycznej pokazujƒÖ wa≈ºno≈õƒá features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE ===\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Feature Importance (Logistic Regression Coefficients)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4.3: Model Comparison\n",
    "\n",
    "**Por√≥wnaj r√≥≈ºne modele klasyfikacyjne**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Przetestuj r√≥≈ºne modele\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# S≈Çownik modeli\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# TODO 1: Trenuj wszystkie modele i zbierz wyniki\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Trenuj\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predykuj\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Ewaluuj\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "# TODO 2: Stw√≥rz DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(results_df)\n",
    "\n",
    "# TODO 3: Wizualizacja por√≥wnania\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(results_df['Model'], results_df['Accuracy'], color='steelblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Model Comparison - Titanic Survival Prediction')\n",
    "plt.xlim([0.7, 0.9])\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    plt.text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Najlepszy model: {results_df.iloc[0]['Model']} z accuracy {results_df.iloc[0]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CZƒò≈öƒÜ 5: Regresja - Boston Housing \n",
    "\n",
    "**Dataset:** Ceny dom√≥w w Bostonie  \n",
    "**Zadanie:** Przewidzieƒá cenƒô domu (regresja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych Boston Housing\n",
    "# Uwaga: load_boston() jest deprecated, u≈ºyjemy alternatywnego ≈∫r√≥d≈Ça\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Nazwy kolumn\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n",
    "                 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "boston_df = pd.DataFrame(data, columns=feature_names)\n",
    "boston_df['PRICE'] = target\n",
    "\n",
    "print(\"Boston Housing Dataset:\")\n",
    "print(boston_df.head())\n",
    "print(f\"\\nShape: {boston_df.shape}\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"RM: Average number of rooms per dwelling\")\n",
    "print(\"LSTAT: % lower status of the population\")\n",
    "print(\"PRICE: Median value of homes (target) in $1000s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 5.1: EDA i Feature Engineering dla Regresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Podstawowe statystyki\n",
    "print(\"Statystyki opisowe:\")\n",
    "print(boston_df.describe())\n",
    "\n",
    "# TODO 2: Sprawd≈∫ missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(boston_df.isnull().sum())\n",
    "\n",
    "# TODO 3: Korelacja z target (PRICE)\n",
    "correlation = boston_df.corr()['PRICE'].sort_values(ascending=False)\n",
    "print(\"\\nKorelacja z PRICE:\")\n",
    "print(correlation)\n",
    "\n",
    "# TODO 4: Heatmap korelacji\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(boston_df.corr(), annot=True, fmt='.2f', cmap='RdYlGn', center=0)\n",
    "plt.title('Correlation Heatmap - Boston Housing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO 5: Scatter plots - najwa≈ºniejsze cechy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RM vs PRICE\n",
    "axes[0].scatter(boston_df['RM'], boston_df['PRICE'], alpha=0.5)\n",
    "axes[0].set_xlabel('RM (Rooms)')\n",
    "axes[0].set_ylabel('PRICE')\n",
    "axes[0].set_title(f'Correlation: {boston_df[\"RM\"].corr(boston_df[\"PRICE\"]):.3f}')\n",
    "\n",
    "# LSTAT vs PRICE\n",
    "axes[1].scatter(boston_df['LSTAT'], boston_df['PRICE'], alpha=0.5, color='red')\n",
    "axes[1].set_xlabel('LSTAT')\n",
    "axes[1].set_ylabel('PRICE')\n",
    "axes[1].set_title(f'Correlation: {boston_df[\"LSTAT\"].corr(boston_df[\"PRICE\"]):.3f}')\n",
    "\n",
    "# PTRATIO vs PRICE\n",
    "axes[2].scatter(boston_df['PTRATIO'], boston_df['PRICE'], alpha=0.5, color='green')\n",
    "axes[2].set_xlabel('PTRATIO')\n",
    "axes[2].set_ylabel('PRICE')\n",
    "axes[2].set_title(f'Correlation: {boston_df[\"PTRATIO\"].corr(boston_df[\"PRICE\"]):.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 5.2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Przygotuj dane\n",
    "X = boston_df.drop('PRICE', axis=1)\n",
    "y = boston_df['PRICE']\n",
    "\n",
    "# TODO 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 3: Skalowanie\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO 4: Trenuj model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TODO 5: Predykcje\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "# TODO 6: Ewaluacja\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Train metrics\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Test metrics\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"=== REGRESSION METRICS ===\")\n",
    "print(f\"\\nTRAIN SET:\")\n",
    "print(f\"  MSE:  {train_mse:.2f}\")\n",
    "print(f\"  RMSE: {train_rmse:.2f}\")\n",
    "print(f\"  MAE:  {train_mae:.2f}\")\n",
    "print(f\"  R¬≤:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  MSE:  {test_mse:.2f}\")\n",
    "print(f\"  RMSE: {test_rmse:.2f}\")\n",
    "print(f\"  MAE:  {test_mae:.2f}\")\n",
    "print(f\"  R¬≤:   {test_r2:.4f}\")\n",
    "\n",
    "# TODO 7: Wizualizacja predykcji vs rzeczywiste\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($1000s)')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title(f'Linear Regression: Actual vs Predicted\\nR¬≤ = {test_r2:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO 8: Residuals (b≈Çƒôdy)\n",
    "residuals = y_test - y_pred_test\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO 9: Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE ===\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PODSUMOWANIE LAB 1.2\n",
    "\n",
    "### Czego siƒô nauczy≈Çe≈õ:\n",
    "\n",
    "‚úÖ **Preprocessing:**\n",
    "- Obs≈Çuga missing values (imputation, deletion)\n",
    "- Wykrywanie i obs≈Çuga outliers (IQR, Z-score)\n",
    "- Analiza i czyszczenie danych\n",
    "\n",
    "‚úÖ **Feature Engineering:**\n",
    "- Kodowanie kategorii (Label, One-Hot)\n",
    "- Tworzenie nowych features\n",
    "- Feature selection\n",
    "\n",
    "‚úÖ **Skalowanie:**\n",
    "- Standardization (StandardScaler)\n",
    "- Normalization (MinMaxScaler)\n",
    "- Kiedy kt√≥rej metody u≈ºyƒá\n",
    "\n",
    "‚úÖ **Machine Learning:**\n",
    "- Scikit-learn API\n",
    "- Train-test split\n",
    "- Logistic Regression (klasyfikacja)\n",
    "- Linear Regression (regresja)\n",
    "- Metryki ewaluacji\n",
    "\n",
    "---\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "1. **Feature Engineering > Algorytm**\n",
    "   - Dobre features mogƒÖ poprawiƒá accuracy o 10-30%\n",
    "   - To wymaga kreatywno≈õci i znajomo≈õci domeny\n",
    "\n",
    "2. **Data Leakage**\n",
    "   - NIGDY nie fit preprocessing na test set\n",
    "   - Train-test split NA POCZƒÑTKU\n",
    "\n",
    "3. **Skalowanie jest kluczowe**\n",
    "   - Dla modeli bazujƒÖcych na odleg≈Ço≈õci\n",
    "   - Dla gradient descent (neural networks)\n",
    "\n",
    "4. **Zawsze waliduj**\n",
    "   - Test set pozostaje nietkniƒôty\n",
    "   - U≈ºywaj cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "### Nastƒôpne zajƒôcia:\n",
    "\n",
    "**Lab 2.1: Wprowadzenie do Sieci Neuronowych**\n",
    "- Perceptron\n",
    "- Forward/Backward propagation\n",
    "- Pierwsza sieƒá neuronowa w NumPy\n",
    "\n",
    "---\n",
    "\n",
    "### Zadanie:\n",
    "\n",
    "1. Doko≈Ñcz wszystkie zadania z dzisiejszych zajƒôƒá i wy≈õlij poprawny notebook\n",
    "2. Eksperymentuj z r√≥≈ºnymi kombinacjami features\n",
    "3. Spr√≥buj ulepszyƒá accuracy powy≈ºej 82% dla Titanic\n",
    "4. Poczytaj o innych technikach feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "**Kontakt:** lukasz.grala@cdv.pl\n",
    "\n",
    "**≈πr√≥d≈Ça danych:**\n",
    "- Kaggle: https://www.kaggle.com/datasets\n",
    "- UCI: https://archive.ics.uci.edu/ml/\n",
    "- Scikit-learn: https://scikit-learn.org/stable/datasets.html\n",
    "\n",
    "**Powodzenia!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
