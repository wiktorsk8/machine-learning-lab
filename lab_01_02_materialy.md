# LAB 1.2: Feature Engineering i Preprocessing Danych

## PREZENTACJA

**Sztuczna Inteligencja - Informatyka, Semestr V**  
**ProwadzÄ…cy:** Åukasz Grala

---

### Slajd 1: Agenda

**Laboratorium 1.2: Feature Engineering & Data Preprocessing**

**Czego siÄ™ nauczysz:**
- âœ… Czym jest Feature Engineering i dlaczego jest kluczowy
- âœ… Techniki preprocessingu danych
- âœ… Kodowanie zmiennych kategorycznych
- âœ… Skalowanie i normalizacja
- âœ… Tworzenie nowych features
- âœ… Feature selection
- âœ… Pierwsze modele ML (scikit-learn)

**NarzÄ™dzia:** Google Colab, Pandas, NumPy, Scikit-learn

---

### Slajd 2: Co to jest Feature Engineering?

**Feature Engineering** = Proces przeksztaÅ‚cania surowych danych w features (cechy), ktÃ³re lepiej reprezentujÄ… problem dla modeli ML

**Dlaczego to waÅ¼ne?**
> "Better data beats more data and better algorithms"
> 
> "Feature engineering is the most important factor in ML competitions" - Kaggle Winners

**PrzykÅ‚ad:**
- Surowe dane: data urodzenia "1995-03-15"
- Po feature engineering:
  - Wiek: 28 lat
  - MiesiÄ…c urodzenia: Marzec (sezonowoÅ›Ä‡)
  - DzieÅ„ tygodnia: Åšroda
  - Czy peÅ‚noletni: True
  - Generacja: Millennial

**Impact:** Dobry feature engineering moÅ¼e zwiÄ™kszyÄ‡ accuracy o 10-30%!

---

### Slajd 3: Proces Machine Learning Pipeline

```
Dane surowe â†’ Preprocessing â†’ Feature Engineering â†’ Model â†’ Predykcja
     â†“             â†“                  â†“              â†“
BrakujÄ…ce    Czyszczenie      Tworzenie nowych   Uczenie
Outliers     Transformacje    cech               Walidacja
Duplikaty    Skalowanie       Selekcja           Tuning
```

**Dzisiejsze zajÄ™cia:** Skupiamy siÄ™ na pierwszych 3 krokach!

**W praktyce:** 80% czasu w projektach ML to preprocessing i feature engineering, tylko 20% to modelowanie

---

### Slajd 4: Typy danych

**1. Numeryczne (Quantitative)**
- **Continuous:** wiek, cena, temperatura (mogÄ… przyjÄ…Ä‡ dowolnÄ… wartoÅ›Ä‡)
- **Discrete:** liczba dzieci, liczba pokoi (tylko liczby caÅ‚kowite)

**2. Kategoryczne (Qualitative)**
- **Nominal:** kolor, miasto, marka (bez kolejnoÅ›ci)
- **Ordinal:** ocena (niska/Å›rednia/wysoka), rozmiar (S/M/L/XL)

**3. Temporalne**
- Data i czas: timestamp, datetime

**4. Tekstowe**
- Opisy, recenzje, dokumenty

**5. Binarne**
- True/False, 0/1, Tak/Nie

**RÃ³Å¼ne typy â†’ rÃ³Å¼ne techniki preprocessingu!**

---

### Slajd 5: BrakujÄ…ce dane (Missing Values)

**Przyczyny:**
- BÅ‚Ä™dy w zbieraniu danych
- Dane niedostÄ™pne
- Dane nie majÄ… sensu (np. dochÃ³d dla dziecka)

**Strategie:**

**1. Usuwanie (Deletion)**
```python
df.dropna()              # UsuÅ„ wiersze z NaN
df.dropna(axis=1)        # UsuÅ„ kolumny z NaN
df.dropna(thresh=5)      # UsuÅ„ jeÅ›li < 5 wartoÅ›ci
```

**2. Imputacja (Imputation)**
```python
# Podstawowa
df.fillna(0)
df.fillna(df.mean())
df.fillna(df.median())

# Zaawansowana
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
```

**3. Predykcja**
- UÅ¼yj ML do przewidzenia brakujÄ…cych wartoÅ›ci

**Kiedy co?**
- < 5% brakujÄ…cych â†’ usuÅ„ wiersze
- 5-40% â†’ imputacja
- \> 40% â†’ usuÅ„ kolumnÄ™ lub uÅ¼yj zaawansowanych metod

---

### Slajd 6: Outliers (WartoÅ›ci odstajÄ…ce)

**Co to jest outlier?**
WartoÅ›Ä‡ znaczÄ…co odbiegajÄ…ca od pozostaÅ‚ych

**Wykrywanie:**

**1. Metoda IQR (Interquartile Range)**
```
Q1 = 25th percentile
Q3 = 75th percentile
IQR = Q3 - Q1
Outlier: < Q1 - 1.5*IQR lub > Q3 + 1.5*IQR
```

**2. Z-score**
```
outlier jeÅ›li |z-score| > 3
```

**3. Wizualne (Box plot)**

**ObsÅ‚uga:**
- **UsuÅ„** - jeÅ›li to bÅ‚Ä…d pomiarowy
- **Zachowaj** - jeÅ›li to prawdziwe ekstremalne wartoÅ›ci
- **Transformuj** - log, sqrt
- **Cap** - ograniczenie do percentyla (np. 95th)

---

### Slajd 7: Kodowanie zmiennych kategorycznych

**Problem:** ML modele dziaÅ‚ajÄ… tylko na liczbach!

**Metody:**

**1. Label Encoding (Ordinal)**
```python
# Dla danych z kolejnoÅ›ciÄ…
size = ['S', 'M', 'L', 'XL']
â†’ [0, 1, 2, 3]
```

**2. One-Hot Encoding (Nominal)**
```python
# Dla danych bez kolejnoÅ›ci
color = ['red', 'blue', 'green']

         red  blue  green
red   â†’   1    0     0
blue  â†’   0    1     0
green â†’   0    0     1
```

**3. Binary Encoding**
Dla zmiennych z wieloma kategoriami (>10)

**4. Target/Mean Encoding**
ZastÄ…p kategoriÄ™ Å›redniÄ… target variable

**âš ï¸ Uwaga:** One-hot moÅ¼e tworzyÄ‡ duÅ¼o kolumn (curse of dimensionality)!

---

### Slajd 8: Skalowanie i Normalizacja

**Dlaczego?**
- RÃ³Å¼ne features majÄ… rÃ³Å¼ne zakresy (wiek: 0-100, dochÃ³d: 0-1000000)
- Modele bazujÄ…ce na odlegÅ‚oÅ›ci (KNN, SVM, Neural Networks) sÄ… wraÅ¼liwe na skalÄ™
- Gradient descent szybciej zbiega dla przeskalowanych danych

**Metody:**

**1. Min-Max Scaling (Normalization)**
```
X_scaled = (X - X_min) / (X_max - X_min)
```
â†’ Zakres [0, 1]

**2. Standardization (Z-score normalization)**
```
X_scaled = (X - Î¼) / Ïƒ
```
â†’ Åšrednia=0, std=1

**3. Robust Scaling**
UÅ¼ywa mediany i IQR (odporny na outliers)

**Kiedy co?**
- **Min-Max:** gdy znamy zakres, nie ma outliers
- **Standardization:** gdy dane majÄ… rozkÅ‚ad normalny
- **Robust:** gdy sÄ… outliers

---

### Slajd 9: Feature Creation (Tworzenie nowych cech)

**1. Z daty/czasu:**
```python
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5,6])
```

**2. Interakcje (Interactions):**
```python
df['BMI'] = df['weight'] / (df['height'] ** 2)
df['price_per_sqm'] = df['price'] / df['area']
```

**3. Binning (Discretization):**
```python
df['age_group'] = pd.cut(df['age'], 
                         bins=[0, 18, 35, 60, 100],
                         labels=['Child', 'Young', 'Adult', 'Senior'])
```

**4. Polynomial Features:**
```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
# X, X^2, X*Y, Y^2
```

**5. Agregacje:**
```python
df['total_spent'] = df.groupby('user_id')['amount'].transform('sum')
```

---

### Slajd 10: Feature Selection

**Dlaczego usuwaÄ‡ features?**
- Zmniejsza overfitting
- Przyspiesza trening
- Upraszcza model (interpretability)
- Curse of dimensionality

**Metody:**

**1. Filter Methods**
- Korelacja z target
- Variance threshold
- Chi-square test
- Mutual information

**2. Wrapper Methods**
- Forward selection
- Backward elimination
- Recursive Feature Elimination (RFE)

**3. Embedded Methods**
- Lasso (L1 regularization)
- Random Forest feature importance
- XGBoost feature importance

**4. Dimensional Reduction**
- PCA (Principal Component Analysis)
- t-SNE, UMAP

---

### Slajd 11: Scikit-learn API

**Scikit-learn** = najpopularniejsza biblioteka ML w Pythonie

**Podstawowy wzorzec:**
```python
from sklearn.xxx import SomeModel

# 1. StwÃ³rz model
model = SomeModel(param1=value1, param2=value2)

# 2. Trenuj (fit)
model.fit(X_train, y_train)

# 3. Predykuj
predictions = model.predict(X_test)

# 4. Ewaluuj
score = model.score(X_test, y_test)
```

**Transformers (preprocessing):**
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Uwaga: tylko transform!
```

**âš ï¸ WAÅ»NE:** NIGDY nie fit na test set!

---

### Slajd 12: Train-Test Split

**Dlaczego?**
Aby oceniÄ‡ jak model dziaÅ‚a na nowych, niewidzianych danych!

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% na test
    random_state=42,    # ReprodukowalnoÅ›Ä‡
    stratify=y          # Zachowaj proporcje klas
)
```

**Typowe podziaÅ‚y:**
- 80/20 (train/test)
- 70/30
- 60/20/20 (train/validation/test)

**Best practice:**
1. Split danych NA POCZÄ„TKU
2. Wszystkie transformacje fit na train
3. Test zostaje nietkniÄ™ty do koÅ„ca

---

### Slajd 13: Pierwszy model - Regresja Liniowa

**Regresja Liniowa** = przewidywanie wartoÅ›ci ciÄ…gÅ‚ej

**RÃ³wnanie:** y = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™

**PrzykÅ‚ady:**
- Przewidywanie ceny mieszkania (na podstawie metraÅ¼u, lokalizacji)
- Przewidywanie temperatury
- Prognoza sprzedaÅ¼y

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Trenuj
model = LinearRegression()
model.fit(X_train, y_train)

# Predykuj
y_pred = model.predict(X_test)

# Ewaluuj
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RÂ²: {r2:.2f}")  # 0-1, bliÅ¼ej 1 = lepiej
```

---

### Slajd 14: Pierwszy model - Regresja Logistyczna

**Regresja Logistyczna** = klasyfikacja (przewidywanie kategorii)

**UÅ¼ywana gdy:** Target jest binarny (0/1, True/False, Yes/No)

**PrzykÅ‚ady:**
- Czy email to spam?
- Czy klient odejdzie? (churn)
- Diagnoza medyczna (chory/zdrowy)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# Trenuj
model = LogisticRegression()
model.fit(X_train, y_train)

# Predykuj
y_pred = model.predict(X_test)

# PrawdopodobieÅ„stwa
probabilities = model.predict_proba(X_test)

# Ewaluuj
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2%}")
```

---

### Slajd 15: Metryki ewaluacji

**Dla Regresji:**

**MSE (Mean Squared Error):**
- Åšrednia kwadratÃ³w bÅ‚Ä™dÃ³w
- Im mniejsze, tym lepiej
- Karze duÅ¼e bÅ‚Ä™dy

**RMSE (Root Mean Squared Error):**
- Pierwiastek z MSE
- W tych samych jednostkach co target

**MAE (Mean Absolute Error):**
- Åšrednia wartoÅ›ci bezwzglÄ™dnych bÅ‚Ä™dÃ³w
- Mniej wraÅ¼liwy na outliers

**RÂ² Score (Coefficient of Determination):**
- 0 do 1 (moÅ¼e byÄ‡ ujemny dla zÅ‚ych modeli)
- 1 = perfekcyjne dopasowanie
- 0 = model nie lepszy niÅ¼ Å›rednia

---

### Slajd 16: Metryki ewaluacji - Klasyfikacja

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
Procent poprawnych predykcji

**Precision (Precyzja):**
```
Precision = TP / (TP + FP)
```
Z przewidzianych pozytywnych, ile jest naprawdÄ™ pozytywnych?

**Recall (CzuÅ‚oÅ›Ä‡, Sensitivity):**
```
Recall = TP / (TP + FN)
```
Z rzeczywistych pozytywnych, ile udaÅ‚o siÄ™ wykryÄ‡?

**F1-Score:**
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```
Åšrednia harmoniczna precision i recall

**Confusion Matrix:**
```
                Predicted
                0       1
Actual    0     TN      FP
          1     FN      TP
```

---

### Slajd 17: Kiedy ktÃ³ra metoda?

**Preprocessing:**
| Problem | RozwiÄ…zanie |
|---------|-------------|
| BrakujÄ…ce dane | Imputacja / usuniÄ™cie |
| Outliers | IQR, Z-score, transformacje |
| RÃ³Å¼ne skale | Standaryzacja / Normalizacja |
| Zmienne kategoryczne | One-hot / Label encoding |
| Zbyt wiele cech | Feature selection / PCA |

**Modele:**
| Zadanie | Model |
|---------|-------|
| Przewidywanie liczby | Linear Regression |
| Klasyfikacja binarna | Logistic Regression |
| Klasyfikacja wieloklasowa | Logistic Regression / Trees |
| Nieliniowe zaleÅ¼noÅ›ci | Polynomial Features + Linear |

---

### Slajd 18: Pipeline w Scikit-learn

**Problem:** DuÅ¼o krokÃ³w preprocessing â†’ Å‚atwo siÄ™ pomyliÄ‡

**RozwiÄ…zanie:** Pipeline!

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Definicja pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

# Wszystko w jednym kroku!
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

**Zalety:**
- Mniej kodu
- Brak wyciekÃ³w danych (data leakage)
- Åatwe do wdroÅ¼enia w produkcji
- MoÅ¼na uÅ¼yÄ‡ w GridSearch

---

### Slajd 19: Best Practices

**âœ… DO:**
- Zawsze rÃ³b train-test split NA POCZÄ„TKU
- Fit preprocessing TYLKO na train
- Zapisuj random_state dla reprodukowalnoÅ›ci
- Wizualizuj dane przed i po preprocessingu
- Dokumentuj wszystkie transformacje
- Zaczynaj od prostych modeli (baseline)

**âŒ DON'T:**
- Nie fituj na test set (data leakage!)
- Nie usuwaj outliers bez analizy
- Nie normalizuj target variable dla regression
- Nie uÅ¼ywaj test accuracy do optymalizacji
- Nie zapomnij o feature scaling dla neural networks

**ZÅ‚ota zasada:**
> "Garbage in, garbage out" - jakoÅ›Ä‡ danych > algorytm

---

### Slajd 20: Zadania praktyczne na dziÅ›

**Zadanie 1:** Czyszczenie i eksploracja danych (20 min)
- ObsÅ‚uga missing values
- Wykrywanie outliers
- Podstawowe statystyki

**Zadanie 2:** Feature Engineering (25 min)
- Kodowanie kategorii
- Tworzenie nowych features
- Skalowanie

**Zadanie 3:** Pierwszy model (25 min)
- Train-test split
- Regresja liniowa
- Regresja logistyczna
- Ewaluacja

**Zadanie 4:** Pipeline (20 min)
- Kompleksowy pipeline
- Cross-validation
- Optymalizacja

**Mini-projekt:** Analiza i predykcja (30 min)
- End-to-end ML project

---

### Slajd 21: Å¹rÃ³dÅ‚a danych do Ä‡wiczeÅ„

**Klasyczne datasety:**
- **Boston Housing** - regresja (ceny mieszkaÅ„)
- **Titanic** - klasyfikacja (survival prediction)
- **Iris** - klasyfikacja (gatunki kwiatÃ³w)
- **Wine Quality** - klasyfikacja/regresja

**Kaggle:**
- House Prices: https://www.kaggle.com/c/house-prices-advanced-regression-techniques
- Titanic: https://www.kaggle.com/c/titanic

**UCI Repository:**
- https://archive.ics.uci.edu/ml/

**Scikit-learn built-in:**
```python
from sklearn.datasets import load_boston, load_iris
data = load_boston()
```

---

### Slajd 22: Przydatne zasoby

**Dokumentacja:**
- Scikit-learn: https://scikit-learn.org/
- Pandas: https://pandas.pydata.org/

**Kursy:**
- Kaggle Learn: https://www.kaggle.com/learn
- Google ML Crash Course

**KsiÄ…Å¼ki:**
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "Feature Engineering for Machine Learning" - Alice Zheng

**Kaggle Competitions:**
- Najlepszy sposÃ³b na naukÄ™ feature engineering!

---

### Slajd 23: Na nastÄ™pne zajÄ™cia

**Lab 2.1: Wprowadzenie do Sieci Neuronowych**

**Przygotuj siÄ™:**
- PowtÃ³rz algebrÄ™ liniowÄ… (mnoÅ¼enie macierzy)
- Podstawy pochodnych (gradient)
- Czym jest funkcja aktywacji

**BÄ™dziemy implementowaÄ‡:**
- Perceptron od zera
- Backpropagation
- PierwszÄ… sieÄ‡ neuronowÄ…

**Do zrobienia:**
- DokoÅ„czyÄ‡ dzisiejsze zadania
- PrzesÅ‚aÄ‡ mini-projekt

**Pytania?** 
ğŸ“§ maksymilian.marcinowski@cdv.pl

---

### Slajd 24: Podsumowanie

**Dzisiaj nauczyÅ‚eÅ› siÄ™:**
âœ… Czym jest feature engineering  
âœ… Jak radziÄ‡ sobie z brakujÄ…cymi danymi  
âœ… Jak wykrywaÄ‡ i obsÅ‚ugiwaÄ‡ outliers  
âœ… Kodowanie zmiennych kategorycznych  
âœ… Skalowanie i normalizacja  
âœ… Tworzenie nowych features  
âœ… Podstawy scikit-learn  
âœ… Pierwszy model ML!  

**Kluczowe wnioski:**
- Feature engineering > algorytm
- Preprocessing to 80% pracy
- Zawsze waliduj na test set
- Pipeline = best practice

**NastÄ™pny krok:** Sieci neuronowe! ğŸ§ 

---

## NOTATKI DLA PROWADZÄ„CEGO

**Timing (120 min):**
- Prezentacja: 45 min
- Zadania praktyczne: 65 min
- Podsumowanie i Q&A: 10 min

**Kluczowe punkty:**
- PodkreÅ›l znaczenie feature engineering (czÄ™sto waÅ¼niejsze niÅ¼ model)
- Pokazuj przykÅ‚ady na Å¼ywych danych
- Demonstruj data leakage i dlaczego jest zÅ‚y
- ZachÄ™caj do eksperymentowania

**Live coding:**
- PokaÅ¼ jak missing values wpÅ‚ywajÄ… na model
- PokaÅ¼ rÃ³Å¼nicÄ™ miÄ™dzy skalowaniem a brakiem skalowania
- Zademonstruj overfitting gdy nie ma train-test split

**Typowe bÅ‚Ä™dy studentÃ³w:**
- Fit preprocessing na caÅ‚ym datasecie (data leakage)
- Zapominanie o random_state
- One-hot encoding bez drop_first (dummy variable trap)
- Skalowanie target variable w regresji

**Interakcja:**
- Pytaj studentÃ³w o ich pomysÅ‚y na features
- Niech eksperymentujÄ… z rÃ³Å¼nymi transformacjami
- Grupowa dyskusja o outliers - usunÄ…Ä‡ czy nie?
- PorÃ³wnanie wynikÃ³w miÄ™dzy studentami

**MateriaÅ‚y dodatkowe:**
- Cheat sheet scikit-learn
- Lista najczÄ™Å›ciej uÅ¼ywanych features
- PrzykÅ‚ady feature engineering z Kaggle

**Na zakoÅ„czenie:**
- PodkreÅ›l Å¼e to praktyczna umiejÄ™tnoÅ›Ä‡
- W prawdziwych projektach spÄ™dzÄ… tu najwiÄ™cej czasu
- Feature engineering to sztuka + nauka
- Wymaga eksperymentowania i kreatywnoÅ›ci
