{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 2.1: Perceptron & Podstawy Sieci Neuronowych\n",
        "\n",
        "**Sztuczna Inteligencja - Semestr V**\n",
        "\n",
        "**ProwadzƒÖcy:** ≈Åukasz Grala\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Cele laboratorium:\n",
        "\n",
        "1. ‚úÖ Zrozumienie podstaw sieci neuronowych\n",
        "2. ‚úÖ Implementacja perceptronu od zera\n",
        "3. ‚úÖ Testowanie na prostych problemach (AND, OR, XOR)\n",
        "4. ‚úÖ Wizualizacja granic decyzyjnych\n",
        "5. ‚úÖ Multi-layer Perceptron dla XOR\n",
        "6. ‚úÖ Pierwsze zastosowanie na rzeczywistych danych (Iris)\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Wymagana wiedza:\n",
        "\n",
        "- Python, NumPy\n",
        "- Podstawy algebry liniowej (mno≈ºenie macierzy)\n",
        "- Pojƒôcie funkcji (input ‚Üí output)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup - Importy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podstawowe biblioteki\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Ustawienia\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"‚úì Biblioteki za≈Çadowane!\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## CZƒò≈öƒÜ 1: Teoria - Co to jest Perceptron?\n",
        "\n",
        "### Czym jest perceptron?\n",
        "\n",
        "**Perceptron** to najprostsza forma sztucznego neuronu, wynaleziona przez Franka Rosenblatta w 1958 roku.\n",
        "\n",
        "**Struktura:**\n",
        "```\n",
        "    x‚ÇÅ ‚îÄ‚îê\n",
        "        ‚îú‚îÄ‚Üí Œ£ (suma wa≈ºona) ‚îÄ‚Üí f(z) ‚îÄ‚Üí output\n",
        "    x‚ÇÇ ‚îÄ‚î§    z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b\n",
        "        ‚îÇ\n",
        "    x‚Çô ‚îÄ‚îò\n",
        "    \n",
        "    gdzie:\n",
        "    - x‚ÇÅ, x‚ÇÇ, ..., x‚Çô : inputs (cechy)\n",
        "    - w‚ÇÅ, w‚ÇÇ, ..., w‚Çô : wagi (weights)\n",
        "    - b : bias (pr√≥g)\n",
        "    - f : funkcja aktywacji\n",
        "    - z : suma wa≈ºona\n",
        "```\n",
        "\n",
        "**Funkcje aktywacji:**\n",
        "\n",
        "1. **Step function** (perceptron klasyczny):\n",
        "   ```\n",
        "   f(z) = 1 je≈õli z >= 0\n",
        "          0 je≈õli z < 0\n",
        "   ```\n",
        "\n",
        "2. **Sigmoid** (logistic):\n",
        "   ```\n",
        "   f(z) = 1 / (1 + e^(-z))\n",
        "   ```\n",
        "\n",
        "3. **ReLU** (Rectified Linear Unit):\n",
        "   ```\n",
        "   f(z) = max(0, z)\n",
        "   ```\n",
        "\n",
        "**Zastosowania:**\n",
        "- Klasyfikacja binarna (2 klasy)\n",
        "- Podstawowy building block dla sieci neuronowych\n",
        "- Rozpoznawanie prostych wzorc√≥w\n",
        "\n",
        "**Ograniczenia:**\n",
        "- Tylko problemy liniowo separowalne\n",
        "- Nie rozwiƒÖ≈ºe XOR!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZƒò≈öƒÜ 2: Implementacja Perceptronu \n",
        "\n",
        "### Zadanie 2.1: Klasa Perceptron\n",
        "\n",
        "Zaimplementuj perceptron z algorytmem uczenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    Perceptron - najprostsza sieƒá neuronowa\n",
        "    \n",
        "    Algorytm uczenia:\n",
        "    1. Inicjalizuj wagi losowo (lub zerami)\n",
        "    2. Dla ka≈ºdego przyk≈Çadu treningowego:\n",
        "        a) Oblicz predykcjƒô: y_pred = f(w¬∑x + b)\n",
        "        b) Oblicz b≈ÇƒÖd: error = y_true - y_pred\n",
        "        c) Zaktualizuj wagi: w = w + learning_rate * error * x\n",
        "        d) Zaktualizuj bias: b = b + learning_rate * error\n",
        "    3. Powtarzaj a≈º do zbie≈ºno≈õci lub max iteracji\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        \"\"\"\n",
        "        Inicjalizacja perceptronu\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        learning_rate : float\n",
        "            Wsp√≥≈Çczynnik uczenia (jak szybko uczymy siƒô)\n",
        "            Typowe warto≈õci: 0.001 - 0.1\n",
        "        n_iters : int\n",
        "            Maksymalna liczba epok (iteracji przez ca≈Çy dataset)\n",
        "        \"\"\"\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trenowanie perceptronu\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape = [n_samples, n_features]\n",
        "            Dane treningowe\n",
        "        y : array-like, shape = [n_samples]\n",
        "            Etykiety (0 lub 1)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # TODO 1: Inicjalizuj wagi zerami\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Konwertuj y do 0/1 je≈õli potrzeba\n",
        "        y_ = np.array(y)\n",
        "        \n",
        "        # TODO 2: G≈Ç√≥wna pƒôtla uczenia\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Forward pass - oblicz predykcjƒô\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "                \n",
        "                # Oblicz b≈ÇƒÖd\n",
        "                error = y_[idx] - y_predicted\n",
        "                \n",
        "                # TODO 3: Update rule - zaktualizuj wagi i bias\n",
        "                # Regu≈Ça: w = w + learning_rate * error * x\n",
        "                self.weights += self.lr * error * x_i\n",
        "                self.bias += self.lr * error\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predykcja dla nowych danych\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape = [n_samples, n_features]\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        y_pred : array, shape = [n_samples]\n",
        "            Predykcje (0 lub 1)\n",
        "        \"\"\"\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted\n",
        "    \n",
        "    def _unit_step_func(self, x):\n",
        "        \"\"\"\n",
        "        Step function - funkcja aktywacji\n",
        "        \n",
        "        Zwraca 1 je≈õli x >= 0, inaczej 0\n",
        "        \"\"\"\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "print(\"‚úì Klasa Perceptron zaimplementowana!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 2.2: Test na AND gate\n",
        "\n",
        "**Bramka AND:**\n",
        "```\n",
        "x1  x2  | output\n",
        "-----------------\n",
        "0   0   |   0\n",
        "0   1   |   0\n",
        "1   0   |   0\n",
        "1   1   |   1\n",
        "```\n",
        "\n",
        "Czy perceptron potrafi nauczyƒá siƒô tego?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Przygotuj dane dla AND gate\n",
        "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "print(\"AND Gate - Dane treningowe:\")\n",
        "print(\"X:\")\n",
        "print(X_and)\n",
        "print(\"\\ny (expected):\")\n",
        "print(y_and)\n",
        "\n",
        "# TODO 2: Stw√≥rz i wytrenuj perceptron\n",
        "p = Perceptron(learning_rate=0.1, n_iters=10)\n",
        "p.fit(X_and, y_and)\n",
        "\n",
        "# TODO 3: Testuj\n",
        "predictions = p.predict(X_and)\n",
        "\n",
        "print(\"\\nPredykcje perceptronu:\")\n",
        "print(predictions)\n",
        "\n",
        "# TODO 4: Sprawd≈∫ accuracy\n",
        "accuracy = np.mean(predictions == y_and)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.1f}%\")\n",
        "\n",
        "# TODO 5: Wy≈õwietl nauczone wagi\n",
        "print(f\"\\nNauczone wagi: {p.weights}\")\n",
        "print(f\"Nauczony bias: {p.bias}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 2.3: Test na OR gate\n",
        "\n",
        "**Bramka OR:**\n",
        "```\n",
        "x1  x2  | output\n",
        "-----------------\n",
        "0   0   |   0\n",
        "0   1   |   1\n",
        "1   0   |   1\n",
        "1   1   |   1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Dane dla OR gate\n",
        "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "\n",
        "print(\"OR Gate - Dane treningowe:\")\n",
        "print(f\"X:\\n{X_or}\")\n",
        "print(f\"\\ny (expected):\\n{y_or}\")\n",
        "\n",
        "# TODO 2: Trenuj perceptron\n",
        "p_or = Perceptron(learning_rate=0.1, n_iters=10)\n",
        "p_or.fit(X_or, y_or)\n",
        "\n",
        "# TODO 3: Predykcje\n",
        "predictions_or = p_or.predict(X_or)\n",
        "print(f\"\\nPredykcje: {predictions_or}\")\n",
        "print(f\"Accuracy: {np.mean(predictions_or == y_or) * 100:.1f}%\")\n",
        "print(f\"\\nWagi: {p_or.weights}, Bias: {p_or.bias}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 2.4: Test na XOR gate - **PROBLEM!**\n",
        "\n",
        "**Bramka XOR:**\n",
        "```\n",
        "x1  x2  | output\n",
        "-----------------\n",
        "0   0   |   0\n",
        "0   1   |   1\n",
        "1   0   |   1\n",
        "1   1   |   0\n",
        "```\n",
        "\n",
        "**Pytanie:** Czy perceptron potrafi nauczyƒá siƒô XOR?\n",
        "\n",
        "**Odpowied≈∫:** **NIE!** XOR nie jest liniowo separowalny.\n",
        "\n",
        "Sprawd≈∫my to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Dane dla XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "print(\"XOR Gate - Dane treningowe:\")\n",
        "print(f\"X:\\n{X_xor}\")\n",
        "print(f\"\\ny (expected):\\n{y_xor}\")\n",
        "\n",
        "# TODO 2: Trenuj perceptron\n",
        "p_xor = Perceptron(learning_rate=0.1, n_iters=100)  # Wiƒôcej iteracji\n",
        "p_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# TODO 3: Predykcje\n",
        "predictions_xor = p_xor.predict(X_xor)\n",
        "print(f\"\\nPredykcje: {predictions_xor}\")\n",
        "print(f\"Expected:  {y_xor}\")\n",
        "print(f\"Accuracy: {np.mean(predictions_xor == y_xor) * 100:.1f}%\")\n",
        "\n",
        "print(\"\\n‚ùå Perceptron NIE MO≈ªE nauczyƒá siƒô XOR!\")\n",
        "print(\"Problem XOR wymaga Multi-Layer Perceptron (MLP)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## CZƒò≈öƒÜ 3: Wizualizacja Granic Decyzyjnych\n",
        "\n",
        "### Zadanie 3.1: Funkcja wizualizujƒÖca\n",
        "\n",
        "Stw√≥rz funkcjƒô kt√≥ra rysuje granicƒô decyzyjnƒÖ perceptronu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
        "    \"\"\"\n",
        "    Rysuje granicƒô decyzyjnƒÖ dla perceptronu (2D)\n",
        "    \n",
        "    Granica decyzyjna to linia gdzie: w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b = 0\n",
        "    \"\"\"\n",
        "    # Stw√≥rz siatkƒô punkt√≥w\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    \n",
        "    xx, yy = np.meshgrid(\n",
        "        np.arange(x_min, x_max, 0.02),\n",
        "        np.arange(y_min, y_max, 0.02)\n",
        "    )\n",
        "    \n",
        "    # Predykcja dla ka≈ºdego punktu siatki\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Rysuj\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=200, edgecolors='black', \n",
        "                cmap='RdYlBu', linewidths=2)\n",
        "    plt.xlabel('x‚ÇÅ', fontsize=12)\n",
        "    plt.ylabel('x‚ÇÇ', fontsize=12)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Rysuj liniƒô decyzyjnƒÖ\n",
        "    # w1*x1 + w2*x2 + b = 0  =>  x2 = -(w1*x1 + b) / w2\n",
        "    if model.weights[1] != 0:\n",
        "        x1_line = np.linspace(x_min, x_max, 100)\n",
        "        x2_line = -(model.weights[0] * x1_line + model.bias) / model.weights[1]\n",
        "        plt.plot(x1_line, x2_line, 'k--', linewidth=2, label='Decision Boundary')\n",
        "        plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Funkcja wizualizacji gotowa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 3.2: Wizualizuj AND, OR i XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Wizualizuj AND gate\n",
        "plot_decision_boundary(X_and, y_and, p, \"AND Gate - Decision Boundary\")\n",
        "\n",
        "# TODO 2: Wizualizuj OR gate\n",
        "plot_decision_boundary(X_or, y_or, p_or, \"OR Gate - Decision Boundary\")\n",
        "\n",
        "# TODO 3: Wizualizuj XOR gate\n",
        "plot_decision_boundary(X_xor, y_xor, p_xor, \"XOR Gate - Perceptron FAILS!\")\n",
        "\n",
        "print(\"\\nüìä Obserwacje:\")\n",
        "print(\"- AND i OR: Liniowo separowalne ‚úì\")\n",
        "print(\"- XOR: Nie da siƒô rozdzieliƒá liniƒÖ prostƒÖ ‚úó\")\n",
        "print(\"- Potrzebujemy wiƒôcej warstw (MLP)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## CZƒò≈öƒÜ 4: Multi-Layer Perceptron dla XOR\n",
        "\n",
        "### Zadanie 4.1: Implementacja MLP\n",
        "\n",
        "**MLP (Multi-Layer Perceptron)** = sieƒá z warstwƒÖ ukrytƒÖ (hidden layer)\n",
        "\n",
        "**Architektura:**\n",
        "```\n",
        "Input (2) ‚Üí Hidden (4, ReLU) ‚Üí Output (1, Sigmoid)\n",
        "```\n",
        "\n",
        "**Forward Propagation:**\n",
        "1. z‚ÇÅ = W‚ÇÅ ¬∑ x + b‚ÇÅ\n",
        "2. a‚ÇÅ = ReLU(z‚ÇÅ)\n",
        "3. z‚ÇÇ = W‚ÇÇ ¬∑ a‚ÇÅ + b‚ÇÇ\n",
        "4. a‚ÇÇ = Sigmoid(z‚ÇÇ)\n",
        "\n",
        "**Backpropagation:**\n",
        "1. Oblicz gradient dla output layer\n",
        "2. Propaguj b≈ÇƒÖd wstecz do hidden layer\n",
        "3. Zaktualizuj wagi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron z 1 hidden layer\n",
        "    \n",
        "    Architecture: Input ‚Üí Hidden (ReLU) ‚Üí Output (Sigmoid)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_input, n_hidden, n_output, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Inicjalizacja MLP\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_input : int\n",
        "            Liczba neuron√≥w wej≈õciowych (features)\n",
        "        n_hidden : int\n",
        "            Liczba neuron√≥w w hidden layer\n",
        "        n_output : int\n",
        "            Liczba neuron√≥w wyj≈õciowych (zwykle 1 dla binary classification)\n",
        "        learning_rate : float\n",
        "            Wsp√≥≈Çczynnik uczenia\n",
        "        \"\"\"\n",
        "        # TODO 1: Inicjalizacja wag (Xavier initialization)\n",
        "        self.W1 = np.random.randn(n_input, n_hidden) * np.sqrt(2. / n_input)\n",
        "        self.b1 = np.zeros((1, n_hidden))\n",
        "        \n",
        "        self.W2 = np.random.randn(n_hidden, n_output) * np.sqrt(2. / n_hidden)\n",
        "        self.b2 = np.zeros((1, n_output))\n",
        "        \n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        # Cache dla backward pass\n",
        "        self.cache = {}\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation: œÉ(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "        # Clip x aby uniknƒÖƒá overflow\n",
        "        x_clipped = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x_clipped))\n",
        "    \n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Pochodna sigmoid: œÉ'(x) = œÉ(x) * (1 - œÉ(x))\"\"\"\n",
        "        return x * (1 - x)\n",
        "    \n",
        "    def relu(self, x):\n",
        "        \"\"\"ReLU activation: ReLU(x) = max(0, x)\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def relu_derivative(self, x):\n",
        "        \"\"\"Pochodna ReLU: 1 je≈õli x > 0, inaczej 0\"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "        \n",
        "        Returns: output\n",
        "        \"\"\"\n",
        "        # TODO 2: Layer 1 (hidden layer)\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        \n",
        "        # TODO 3: Layer 2 (output layer)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"\n",
        "        Backpropagation - oblicza gradienty i aktualizuje wagi\n",
        "        \n",
        "        Wzory:\n",
        "        - Output layer: dL/dW2 = a1.T @ (output - y)\n",
        "        - Hidden layer: dL/dW1 = X.T @ (dz1)\n",
        "            gdzie dz1 = (output - y) @ W2.T * relu'(a1)\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # TODO 4: Gradienty dla output layer\n",
        "        # Dla Binary Cross-Entropy + Sigmoid, gradient = output - y\n",
        "        dz2 = output - y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "        \n",
        "        # TODO 5: Gradienty dla hidden layer\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.relu_derivative(self.a1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "        \n",
        "        # TODO 6: Update wag (gradient descent)\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "    \n",
        "    def train(self, X, y, epochs=10000, verbose=True):\n",
        "        \"\"\"\n",
        "        Trenowanie sieci\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array, shape = [n_samples, n_features]\n",
        "        y : array, shape = [n_samples, 1]\n",
        "        epochs : int\n",
        "            Liczba epok\n",
        "        verbose : bool\n",
        "            Czy wy≈õwietlaƒá progress\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        losses : list\n",
        "            Historia funkcji straty\n",
        "        \"\"\"\n",
        "        losses = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "            \n",
        "            # Oblicz loss (Binary Cross-Entropy)\n",
        "            loss = -np.mean(y * np.log(output + 1e-8) + \n",
        "                           (1 - y) * np.log(1 - output + 1e-8))\n",
        "            losses.append(loss)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.backward(X, y, output)\n",
        "            \n",
        "            # Wy≈õwietl progress\n",
        "            if verbose and epoch % 1000 == 0:\n",
        "                print(f\"Epoch {epoch:5d}, Loss: {loss:.4f}\")\n",
        "        \n",
        "        return losses\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predykcja (0 lub 1)\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)\n",
        "\n",
        "print(\"‚úì Klasa MLP zaimplementowana!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 4.2: RozwiƒÖzanie XOR z MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dane XOR\n",
        "X_xor_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "print(\"XOR - Dane treningowe:\")\n",
        "print(f\"X:\\n{X_xor_train}\")\n",
        "print(f\"\\ny:\\n{y_xor_train.flatten()}\")\n",
        "\n",
        "# TODO 1: Stw√≥rz MLP\n",
        "mlp = MLP(n_input=2, n_hidden=4, n_output=1, learning_rate=0.1)\n",
        "\n",
        "print(\"\\nüîÑ Trening MLP dla XOR...\")\n",
        "# TODO 2: Trenuj\n",
        "losses = mlp.train(X_xor_train, y_xor_train, epochs=10000, verbose=True)\n",
        "\n",
        "# TODO 3: Testuj\n",
        "predictions = mlp.predict(X_xor_train)\n",
        "print(f\"\\n=== WYNIKI ===\")\n",
        "print(f\"Predykcje: {predictions.flatten()}\")\n",
        "print(f\"Expected:  {y_xor_train.flatten()}\")\n",
        "print(f\"Accuracy:  {np.mean(predictions == y_xor_train) * 100:.1f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ MLP ROZWIƒÑZA≈Å XOR!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 4.3: Wizualizacja uczenia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses, linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
        "plt.title('Learning Curve - MLP training on XOR', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìâ Loss maleje - sieƒá siƒô uczy!\")\n",
        "\n",
        "# TODO 2: Wizualizuj granicƒô decyzyjnƒÖ MLP\n",
        "def plot_mlp_boundary(X, y, model, title=\"MLP Decision Boundary\"):\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    \n",
        "    xx, yy = np.meshgrid(\n",
        "        np.arange(x_min, x_max, 0.02),\n",
        "        np.arange(y_min, y_max, 0.02)\n",
        "    )\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=1)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=200, edgecolors='black',\n",
        "                cmap='RdYlBu', linewidths=2)\n",
        "    plt.xlabel('x‚ÇÅ', fontsize=12)\n",
        "    plt.ylabel('x‚ÇÇ', fontsize=12)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Wizualizuj\n",
        "plot_mlp_boundary(X_xor_train, y_xor_train.flatten(), mlp, \n",
        "                  \"XOR - MLP Decision Boundary (NON-LINEAR)\")\n",
        "\n",
        "print(\"üé® MLP tworzy nieliniowƒÖ granicƒô decyzyjnƒÖ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## CZƒò≈öƒÜ 5: Zastosowanie na Iris Dataset\n",
        "\n",
        "### Zadanie 5.1: Wczytaj i przygotuj dane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wczytaj Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data[:, :2]  # U≈ºyjemy tylko 2 pierwszych cech (dla wizualizacji)\n",
        "y_iris = (iris.target == 0).astype(int).reshape(-1, 1)  # Binary: setosa vs rest\n",
        "\n",
        "print(\"Iris Dataset:\")\n",
        "print(f\"Features shape: {X_iris.shape}\")\n",
        "print(f\"Target shape: {y_iris.shape}\")\n",
        "print(f\"\\nKlasy (pierwsze 10):\")\n",
        "print(y_iris[:10].flatten())\n",
        "\n",
        "# TODO 1: Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# TODO 2: Skalowanie\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train_scaled.shape}\")\n",
        "print(f\"Test set: {X_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 5.2: Trenuj MLP na Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Stw√≥rz MLP\n",
        "mlp_iris = MLP(n_input=2, n_hidden=8, n_output=1, learning_rate=0.01)\n",
        "\n",
        "print(\"üîÑ Trening MLP na Iris dataset...\")\n",
        "# TODO 2: Trenuj\n",
        "losses_iris = mlp_iris.train(X_train_scaled, y_train, epochs=5000, verbose=False)\n",
        "\n",
        "print(\"‚úì Trening zako≈Ñczony!\")\n",
        "\n",
        "# TODO 3: Ewaluacja na train set\n",
        "train_pred = mlp_iris.predict(X_train_scaled)\n",
        "train_accuracy = np.mean(train_pred == y_train) * 100\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# TODO 4: Ewaluacja na test set\n",
        "test_pred = mlp_iris.predict(X_test_scaled)\n",
        "test_accuracy = np.mean(test_pred == y_test) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# TODO 5: Learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses_iris, linewidth=2, color='blue')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Learning Curve - Iris Dataset', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 5.3: Wizualizacja wynik√≥w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Scatter plot - actual vs predicted\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Train set\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_scaled[y_train.flatten() == 0, 0], \n",
        "           X_train_scaled[y_train.flatten() == 0, 1],\n",
        "           c='blue', label='Class 0', s=100, alpha=0.6, edgecolors='black')\n",
        "plt.scatter(X_train_scaled[y_train.flatten() == 1, 0], \n",
        "           X_train_scaled[y_train.flatten() == 1, 1],\n",
        "           c='red', label='Class 1', s=100, alpha=0.6, edgecolors='black')\n",
        "plt.xlabel('Sepal Length (scaled)', fontsize=12)\n",
        "plt.ylabel('Sepal Width (scaled)', fontsize=12)\n",
        "plt.title(f'Train Set (Accuracy: {train_accuracy:.1f}%)', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Test set\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_scaled[y_test.flatten() == 0, 0], \n",
        "           X_test_scaled[y_test.flatten() == 0, 1],\n",
        "           c='blue', label='Class 0', s=100, alpha=0.6, edgecolors='black')\n",
        "plt.scatter(X_test_scaled[y_test.flatten() == 1, 0], \n",
        "           X_test_scaled[y_test.flatten() == 1, 1],\n",
        "           c='red', label='Class 1', s=100, alpha=0.6, edgecolors='black')\n",
        "plt.xlabel('Sepal Length (scaled)', fontsize=12)\n",
        "plt.ylabel('Sepal Width (scaled)', fontsize=12)\n",
        "plt.title(f'Test Set (Accuracy: {test_accuracy:.1f}%)', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO 2: Decision boundary\n",
        "plot_mlp_boundary(X_test_scaled, y_test.flatten(), mlp_iris,\n",
        "                  \"Iris Dataset - MLP Decision Boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PODSUMOWANIE LAB 2.1\n",
        "\n",
        "### Czego siƒô nauczy≈Çe≈õ:\n",
        "\n",
        "‚úÖ **Teoria:**\n",
        "- Czym jest perceptron\n",
        "- Jak dzia≈Ça algorytm uczenia\n",
        "- Pojƒôcie funkcji aktywacji\n",
        "- Liniowa separowalno≈õƒá\n",
        "\n",
        "‚úÖ **Praktyka:**\n",
        "- Implementacja perceptronu od zera\n",
        "- Testowanie na AND, OR, XOR\n",
        "- Wizualizacja granic decyzyjnych\n",
        "- Problem XOR i jego rozwiƒÖzanie\n",
        "\n",
        "‚úÖ **MLP:**\n",
        "- Multi-layer perceptron\n",
        "- Forward i backward propagation\n",
        "- Nieliniowe granice decyzyjne\n",
        "- Zastosowanie na rzeczywistych danych\n",
        "\n",
        "---\n",
        "\n",
        "### Kluczowe wnioski:\n",
        "\n",
        "1. **Perceptron** mo≈ºe rozwiƒÖzaƒá tylko problemy **liniowo separowalne**\n",
        "2. **XOR** wymaga sieci wielowarstwowej (MLP)\n",
        "3. **Hidden layer** pozwala nauczyƒá siƒô nieliniowych wzorc√≥w\n",
        "4. **Backpropagation** to kluczowy algorytm uczenia g≈Çƒôbokich sieci\n",
        "\n",
        "---\n",
        "\n",
        "### Na nastƒôpne zajƒôcia (Lab 2.2):\n",
        "\n",
        "**Temat:** MLP & Backpropagation (szczeg√≥≈Çowo)\n",
        "\n",
        "**Bƒôdziemy omawiaƒá:**\n",
        "- Matematyka backpropagation krok po kroku\n",
        "- R√≥≈ºne optimizers (SGD, Momentum, Adam)\n",
        "- Weight initialization\n",
        "- Problem znikajƒÖcego gradientu\n",
        "- Mini-batch training\n",
        "- MNIST classification\n",
        "\n",
        "**Przygotuj siƒô:**\n",
        "- Powt√≥rz pochodne (chain rule)\n",
        "- Gradient descent\n",
        "- Matrix multiplication\n",
        "\n",
        "---\n",
        "\n",
        "### Zadanie:\n",
        "\n",
        "1. ‚úÖ Eksperymentuj z r√≥≈ºnymi learning rates dla MLP\n",
        "2. ‚úÖ Spr√≥buj r√≥≈ºnej liczby neuron√≥w w hidden layer (2, 4, 8, 16)\n",
        "3. ‚úÖ U≈ºyj wszystkich 4 cech z Iris (zamiast 2)\n",
        "4. ‚úÖ Zaimplementuj perceptron dla multi-class classification (3 klasy Iris)\n",
        "5. ‚úÖ Poczytaj o backpropagation przed nastƒôpnymi zajƒôciami\n",
        "\n",
        "---\n",
        "\n",
        "**Kontakt:** lukasz.grala@cdv.pl\n",
        "\n",
        "**Materia≈Çy:**\n",
        "- Nielsen, M.: \"Neural Networks and Deep Learning\" (darmowy online)\n",
        "- 3Blue1Brown: \"Neural Networks\" (YouTube)\n",
        "- Coursera: Andrew Ng \"Machine Learning\"\n",
        "\n",
        "**Powodzenia!** üöÄüß†\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
