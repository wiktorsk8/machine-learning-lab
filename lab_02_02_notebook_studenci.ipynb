{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 2.2: MLP & Backpropagation\n",
        "\n",
        "**Czas:** 2h | **Temat:** Szczegółowa matematyka sieci neuronowych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Setup complete\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "print('✓ Setup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 1: Backpropagation - Matematyka\n",
        "\n",
        "### Forward Pass:\n",
        "```\n",
        "z₁ = W₁x + b₁\n",
        "a₁ = σ(z₁)\n",
        "z₂ = W₂a₁ + b₂\n",
        "a₂ = σ(z₂)\n",
        "ŷ = a₂\n",
        "```\n",
        "\n",
        "### Backward Pass (Chain Rule):\n",
        "```\n",
        "∂L/∂W₂ = ∂L/∂a₂ · ∂a₂/∂z₂ · ∂z₂/∂W₂\n",
        "∂L/∂W₁ = ∂L/∂a₂ · ∂a₂/∂z₂ · ∂z₂/∂a₁ · ∂a₁/∂z₁ · ∂z₁/∂W₁\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 1: Implementacja różnych optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement SGD, Momentum, Adam\n",
        "class Optimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            # Standardowy wzór SGD: w = w - lr * grad\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "class MomentumOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.v = {}\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        v = momentum * v - lr * grad\n",
        "        params += v\n",
        "\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        m = beta1 * m + (1-beta1) * grad\n",
        "        v = beta2 * v + (1-beta2) * grad^2\n",
        "        m_hat = m / (1-beta1^t)\n",
        "        v_hat = v / (1-beta2^t)\n",
        "        params -= lr * m_hat / (sqrt(v_hat) + eps)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 2: Mini-batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement mini-batch gradient descent\n",
        "def create_mini_batches(X, y, batch_size=32):\n",
        "    mini_batches = []\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffled = X[indices]\n",
        "    y_shuffled = y[indices]\n",
        "    \n",
        "    n_minibatches = X.shape[0] // batch_size\n",
        "    \n",
        "    for i in range(n_minibatches):\n",
        "        X_batch = X_shuffled[i * batch_size : (i + 1) * batch_size]\n",
        "        y_batch = y_shuffled[i * batch_size : (i + 1) * batch_size]\n",
        "        mini_batches.append((X_batch, y_batch))\n",
        "        \n",
        "    if X.shape[0] % batch_size != 0:\n",
        "        X_batch = X_shuffled[n_minibatches * batch_size :]\n",
        "        y_batch = y_shuffled[n_minibatches * batch_size :]\n",
        "        mini_batches.append((X_batch, y_batch))\n",
        "        \n",
        "    return mini_batches\n",
        "\n",
        "# TODO: Train with mini-batches\n",
        "# for epoch in range(n_epochs):\n",
        "#     for X_batch, y_batch in mini_batches:\n",
        "#         # forward + backward + update\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 3: MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (70000, 784)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'].values, mnist['target'].values.astype(int)\n",
        "\n",
        "# A. Normalize to [0,1]\n",
        "X = X / 255.0\n",
        "\n",
        "# B. One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# C. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_one_hot, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Dataset shape: {X.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPClassifier:\n",
        "    def __init__(self, layers=[784, 128, 64, 10], lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(layers[0], layers[1]) * 0.1,\n",
        "            'b1': np.zeros((1, layers[1])),\n",
        "\n",
        "            'W2': np.random.randn(layers[1], layers[2]) * 0.1,\n",
        "            'b2': np.zeros((1, layers[2])),\n",
        "\n",
        "            'W3': np.random.randn(layers[2], layers[3]) * 0.1,\n",
        "            'b3': np.zeros((1, layers[3]))\n",
        "        }\n",
        "        self.optimizer = Optimizer(learning_rate=self.lr)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
        "        a1 = self._sigmoid(z1)\n",
        "        \n",
        "        z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
        "        a2 = self._sigmoid(z2)\n",
        "        \n",
        "        z3 = np.dot(a2, self.params['W3']) + self.params['b3']\n",
        "        a3 = self._sigmoid(z3)\n",
        "        \n",
        "        return {\n",
        "            'z1': z1, 'a1': a1,\n",
        "            'z2': z2, 'a2': a2,\n",
        "            'z3': z3, 'a3': a3\n",
        "        }\n",
        "    \n",
        "    def train(self, X, y, epochs=10, batch_size=32):\n",
        "        loss_history = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # 1. Tworzymy mini-batche\n",
        "            batches = create_mini_batches(X, y, batch_size)\n",
        "            epoch_loss = 0\n",
        "            \n",
        "            for X_batch, y_batch in batches:\n",
        "                # 2. Forward pass\n",
        "                activations = self.forward(X_batch)\n",
        "                y_pred = activations['a3']\n",
        "                \n",
        "                # 3. Obliczamy błąd (MSE)\n",
        "                batch_loss = np.mean((y_batch - y_pred)**2)\n",
        "                epoch_loss += batch_loss\n",
        "                \n",
        "                # 4. Backward pass (obliczanie gradientów)\n",
        "                # Załóżmy, że metoda backward jest zaimplementowana wg reguły łańcuchowej\n",
        "                grads = self.backward(X_batch, y_batch, activations)\n",
        "                \n",
        "                # 5. Aktualizacja wag\n",
        "                self.optimizer.update(self.params, grads)\n",
        "            \n",
        "            avg_loss = epoch_loss / len(batches)\n",
        "            loss_history.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "            \n",
        "        return loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PODSUMOWANIE\n",
        "\n",
        "✅ Backpropagation szczegółowo\n",
        "✅ Różne optimizers\n",
        "✅ Mini-batch training\n",
        "✅ MNIST classification\n",
        "\n",
        "\n",
        "Podsumowanie: praktyczna implementacja teorii i matematyki stojącej za modelami uczenia maszynowego. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
