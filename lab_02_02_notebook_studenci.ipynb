{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 2.2: MLP & Backpropagation\n",
        "\n",
        "**Czas:** 2h | **Temat:** Szczegółowa matematyka sieci neuronowych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "print('✓ Setup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 1: Backpropagation - Matematyka\n",
        "\n",
        "### Forward Pass:\n",
        "```\n",
        "z₁ = W₁x + b₁\n",
        "a₁ = σ(z₁)\n",
        "z₂ = W₂a₁ + b₂\n",
        "a₂ = σ(z₂)\n",
        "ŷ = a₂\n",
        "```\n",
        "\n",
        "### Backward Pass (Chain Rule):\n",
        "```\n",
        "∂L/∂W₂ = ∂L/∂a₂ · ∂a₂/∂z₂ · ∂z₂/∂W₂\n",
        "∂L/∂W₁ = ∂L/∂a₂ · ∂a₂/∂z₂ · ∂z₂/∂a₁ · ∂a₁/∂z₁ · ∂z₁/∂W₁\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zadanie 1: Implementacja różnych optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement SGD, Momentum, Adam\n",
        "class Optimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        # TODO: Implement SGD\n",
        "        pass\n",
        "\n",
        "class MomentumOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.v = {}\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        # TODO: v = momentum * v - lr * grad\n",
        "        # TODO: params += v\n",
        "        pass\n",
        "\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        # TODO: Implement Adam\n",
        "        # m = beta1 * m + (1-beta1) * grad\n",
        "        # v = beta2 * v + (1-beta2) * grad²\n",
        "        # m_hat = m / (1-beta1^t)\n",
        "        # v_hat = v / (1-beta2^t)\n",
        "        # params -= lr * m_hat / (sqrt(v_hat) + eps)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 2: Mini-batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement mini-batch gradient descent\n",
        "def create_mini_batches(X, y, batch_size=32):\n",
        "    # TODO: Split data into mini-batches\n",
        "    pass\n",
        "\n",
        "# TODO: Train with mini-batches\n",
        "# for epoch in range(n_epochs):\n",
        "#     for X_batch, y_batch in mini_batches:\n",
        "#         # forward + backward + update\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CZĘŚĆ 3: MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load MNIST\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'].values, mnist['target'].values.astype(int)\n",
        "\n",
        "# TODO: Preprocess\n",
        "# - Normalize to [0,1]\n",
        "# - One-hot encode labels\n",
        "# - Train-test split\n",
        "\n",
        "print(f'Dataset shape: {X.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build MLP for MNIST (784 -> 128 -> 64 -> 10)\n",
        "class MLPClassifier:\n",
        "    def __init__(self):\n",
        "        # TODO: Initialize layers\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # TODO: Forward pass\n",
        "        pass\n",
        "    \n",
        "    def train(self, X, y, epochs=10, batch_size=32):\n",
        "        # TODO: Training loop\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PODSUMOWANIE\n",
        "\n",
        "✅ Backpropagation szczegółowo\n",
        "✅ Różne optimizers\n",
        "✅ Mini-batch training\n",
        "✅ MNIST classification"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
